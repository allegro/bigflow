{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The BigFlow tutorial part 1\n",
    "\n",
    "### What you will learn in part 1\n",
    "* Making queries\n",
    "* Writing to tables\n",
    "* Creating tables\n",
    "* Creating workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install bigflow==0.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your machine configuration it can also be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bigflow==0.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bigflow as bf\n",
    "import pydata_google_auth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task definition\n",
    "To guide you through the most important features that BigFlow provides, we've prepared a simple task. There is a **transactions** table, which looks like this:\n",
    "\n",
    "| user_id | transaction_value | partition_timestamp |\n",
    "|---------|-------------------|---------------------|\n",
    "| john123 | 800               | 2019-01-01 00:00:00 |\n",
    "| smith99 | 10000             | 2019-01-01 00:00:00 |\n",
    "| smith99 | 30000             | 2019-01-01 00:00:00 |\n",
    "\n",
    "The table contains all transactions that users make on a specific day. Your task is to calculate two metrics for each user:\n",
    " daily user transaction value and daily user transaction count.\n",
    "\n",
    "Final result should be a table named **user_transaction_metrics**:\n",
    "\n",
    "| user_id | metric_value | metric_name            | partition_timestamp |\n",
    "|---------|--------------|------------------------|---------------------|\n",
    "| john123 | 800          | USER_TRANSACTION_VALUE | 2019-01-01 00:00:00 |\n",
    "| smith99 | 40000        | USER_TRANSACTION_VALUE | 2019-01-01 00:00:00 |\n",
    "| john123 | 1            | USER_TRANSACTION_COUNT | 2019-01-01 00:00:00 |\n",
    "| smith99 | 2            | USER_TRANSACTION_COUNT | 2019-01-01 00:00:00 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing transactions table\n",
    "Before you start processing the transactions table, you need to create it. Execute the cell below to create the transactions table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = ''  # put your own project ID\n",
    "\n",
    "# If you are using this code in jupyter lab environment you should not use these credentials but service account and set below variable credentials to None.\n",
    "# If you are using this code on your local machine you should leave credentials variable as it is.\n",
    "\n",
    "# You will be asked to copy the url to your browser and then login using your credentials. Then copy a token and paste it in jupyter.\n",
    "credentials = pydata_google_auth.get_user_credentials(['https://www.googleapis.com/auth/bigquery'])  \n",
    "\n",
    "dataset = bf.Dataset(project_id=PROJECT_ID, dataset_name='transactions', credentials=credentials)\n",
    "dataset.create_table(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS transactions (\n",
    "    user_id STRING,\n",
    "    transaction_value FLOAT64,\n",
    "    partition_timestamp TIMESTAMP)\n",
    "PARTITION BY DATE(partition_timestamp)\"\"\").run()\n",
    "\n",
    "dataset.write_truncate('transactions', \"\"\"\n",
    "SELECT 'john123' as user_id, 800.0 as transaction_value, TIMESTAMP('2019-01-01') as partition_timestamp\n",
    "\"\"\").run('2019-01-01')\n",
    "dataset.write_append('transactions', \"\"\"\n",
    "SELECT 'smith99' as user_id, 10000.0 as transaction_value, TIMESTAMP('2019-01-01') as partition_timestamp\n",
    "\"\"\").run('2019-01-01')\n",
    "dataset.write_append('transactions', \"\"\"\n",
    "SELECT 'smith99' as user_id, 30000.0 as transaction_value, TIMESTAMP('2019-01-01') as partition_timestamp\n",
    "\"\"\").run('2019-01-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset object\n",
    "Using the dataset object you can manipulate tables inside the specified dataset. You need to put names of the tables that you want to manipulate into the `internal_tables` parameter. The `external_tables` parameter lets you access external tables inside your queries using an alias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = bf.Dataset(\n",
    "    project_id=PROJECT_ID,\n",
    "    dataset_name='transaction_aggregates',\n",
    "    internal_tables=['user_transaction_metrics'],\n",
    "    external_tables={\n",
    "        'transactions': '{}.transactions.transactions'.format(PROJECT_ID)\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The create_table method\n",
    "To store the metrics, you need to create the **user_transaction_metrics** table. The `create_table` method returns the component that you can run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_user_transaction_metrics_table = dataset.create_table('''\n",
    "CREATE TABLE IF NOT EXISTS user_transaction_metrics (\n",
    "    user_id STRING,\n",
    "    metric_name STRING,\n",
    "    metric_value STRING,\n",
    "    \n",
    "    partition_timestamp TIMESTAMP)\n",
    "PARTITION BY DATE(partition_timestamp)\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_user_transaction_metrics_table.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple queries\n",
    "Take a look at the transactions. You can do that using the `collect` method. The `collect` method returns a component that you can `peek`.\n",
    "The `peek` method returns the query result in a DataFrame object.\n",
    "\n",
    "In every operation, you can access run time using `dt` alias and all the tables you've specified in your dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_transactions_from_single_day = dataset.collect('''\n",
    "SELECT *\n",
    "FROM `{transactions}`\n",
    "WHERE DATE(partition_timestamp) = '{dt}'\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_transactions_from_single_day.peek('2019-01-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Estimating the cost of a query\n",
    "The dry_run method returns a string with the estimated number of bytes read by the query and estimated cost of a query in dollars."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "costs = dataset.dry_run('''\n",
    "        SELECT *\n",
    "        FROM `{transactions}`\n",
    "        WHERE DATE(partition_timestamp) = '{dt}'\n",
    "        ''')\n",
    "costs.run()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the first metric using the write_truncate method\n",
    "The `write_truncate` method saves the result of the query to the table (to the specified partition) and overrides any old content. Before running the component for real, you can\n",
    "`peek` the query result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_truncate_user_transaction_value = dataset.write_truncate('user_transaction_metrics', \n",
    "'''\n",
    "SELECT \n",
    "    TIMESTAMP('{dt}') as partition_timestamp,\n",
    "    user_id,\n",
    "    CAST(sum(CAST(transaction_value as FLOAT64)) as string) as metric_value,\n",
    "    'USER_TRANSACTION_VALUE' as metric_name\n",
    "FROM `{transactions}`\n",
    "WHERE DATE(partition_timestamp) = '{dt}'\n",
    "GROUP BY user_id\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_truncate_user_transaction_value.peek('2019-01-01', limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_truncate_user_transaction_value.run('2019-01-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the second metric using the write_append method\n",
    "The only difference between the `write_append` and `write_truncate` is that `write_append` does not override old content. You can use that behaviour to add the second metric\n",
    "to the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_append_user_transaction_count = dataset.write_append('user_transaction_metrics', \n",
    "'''\n",
    "SELECT \n",
    "    TIMESTAMP('{dt}') as partition_timestamp,\n",
    "    user_id,\n",
    "    CAST(count(*) as string) as metric_value,\n",
    "    'USER_TRANSACTION_COUNT' as metric_name\n",
    "FROM `{transactions}`\n",
    "WHERE DATE(partition_timestamp) = '{dt}'\n",
    "GROUP BY user_id\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_append_user_transaction_count.peek('2019-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_append_user_transaction_count.run('2019-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.collect('''\n",
    "SELECT *\n",
    "FROM `{user_transaction_metrics}`\n",
    "WHERE date(partition_timestamp) = '{dt}'\n",
    "''').peek('2019-01-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the workflow\n",
    "Finally, create the workflow using components that you created. You can `run` the workflow just like a component. **To deploy your workflow, go to the tutorial part 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = bf.Workflow(definition=[\n",
    "    create_user_transaction_metrics_table.to_job(),\n",
    "    write_truncate_user_transaction_value.to_job(),\n",
    "    write_append_user_transaction_count.to_job()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.run('2019-01-02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}