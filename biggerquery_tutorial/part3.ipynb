{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The BiggerQuery tutorial part 3\n",
    "\n",
    "### What you will lean in the part 3\n",
    "* Creating custom components.\n",
    "* Creating table sensors and checks.\n",
    "* Loading pandas DataFrame to BQ table.\n",
    "* Setting custom schedule interval to workflow.\n",
    "* Setting custom number of retries to workflow step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import biggerquery as bgq\n",
    "\n",
    "dataset = bgq.Dataset(\n",
    "    project_id='',  # put you own project ID\n",
    "    dataset_name='transaction_aggregates',\n",
    "    internal_tables=['user_transaction_metrics', 'global_transaction_metrics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beyond queries\n",
    "You can create, run and schedule components that are python code. You can do pretty much anything in them, same things you can do in a jupyter notebook. \n",
    "\n",
    "As you can see on the example below:\n",
    "* Custom component takes a dataset as a argument.\n",
    "* Inside custom component you can use all standard dataset methods(`create_table`, `write_truncate`, ...).\n",
    "* You don't need to run components inside a custom component- BiggerQuery will run them with a proper date automatically.\n",
    "* You can use method `load_table_from_dataframe` to save specified dataframe to a table. It works with both partitioned and non-partitioned tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "@bgq.component(ds=dataset)\n",
    "def calculate_user_transaction_value_median_and_save_to_table(ds):\n",
    "    ds.create_table('''\n",
    "    CREATE TABLE IF NOT EXISTS global_transaction_metrics (\n",
    "        metric_name STRING,\n",
    "        metric_value STRING,\n",
    "\n",
    "        partition_timestamp TIMESTAMP)\n",
    "    PARTITION BY DATE(partition_timestamp)\n",
    "    ''', operation_name='create_table')\n",
    "    \n",
    "    daily_user_transaction_cost_dataframe = ds.collect('''\n",
    "    SELECT CAST(metric_value as FLOAT64) as metric_value,\n",
    "        partition_timestamp\n",
    "    FROM `{user_transaction_metrics}`\n",
    "    WHERE DATE(partition_timestamp) = DATE('{dt}')\n",
    "    AND metric_name = 'USER_TRANSACTION_VALUE'\n",
    "    ''', operation_name='fetch_metrics')\n",
    "    \n",
    "    metric_median = daily_user_transaction_cost_dataframe.loc[:,'metric_value'].median()\n",
    "    metric_median_dataframe = pd.DataFrame([{\n",
    "        'metric_name': 'USER_TRANSACTION_VALUE_MEDIAN',\n",
    "        'metric_value': str(metric_median),\n",
    "        'partition_timestamp': pd.Timestamp(ds.dt, tz='UTC')\n",
    "    }])\n",
    "    \n",
    "    ds.load_table_from_dataframe('global_transaction_metrics', metric_median_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom component methods \n",
    "Custom component acts like a components that you created in the previous tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_user_transaction_value_median_and_save_to_table.run('2019-01-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can peek and run a single operation inside your custom component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_user_transaction_value_median_and_save_to_table.peek('2019-01-01', operation_name='fetch_metrics', limit=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How it works\n",
    "Creating a inline component is just shortcut for creating a custom component. When you do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_global_daily_transaction_metrics_table = dataset.create_table('''\n",
    "CREATE TABLE IF NOT EXISTS global_daily_transaction_metrics (\n",
    "    metric_name STRING,\n",
    "    metric_value STRING,\n",
    "\n",
    "    partition_timestamp TIMESTAMP)\n",
    "PARTITION BY DATE(partition_timestamp)\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You implicitly do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@bgq.component(ds=dataset)\n",
    "def create_global_daily_transaction_metrics_table(ds):\n",
    "    ds.create_table('''\n",
    "    CREATE TABLE IF NOT EXISTS global_daily_transaction_metrics (\n",
    "        metric_name STRING,\n",
    "        metric_value STRING,\n",
    "\n",
    "        partition_timestamp TIMESTAMP)\n",
    "    PARTITION BY DATE(partition_timestamp)\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making you own table check/sensor\n",
    "What if you want to wait for some table, before you start your processing? You need some component that checks if table you depend on is ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@bgq.component(ds=dataset)\n",
    "def user_transaction_metrics_is_ready(ds):\n",
    "    result = ds.collect('''\n",
    "    SELECT count(*) > 0 as table_ready\n",
    "    FROM `{user_transaction_metrics}`\n",
    "    WHERE DATE(partition_timestamp) = DATE('{dt}')\n",
    "    ''')\n",
    "\n",
    "    if not result.iloc[0]['table_ready']:\n",
    "        raise ValueError('user_transaction_metrics is not ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should throw ValueError\n",
    "user_transaction_metrics_is_ready.run('2022-01-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making use of the check\n",
    "Lets create workflow that uses the `user_transaction_metrics_is_ready` component and the `calculate_user_transaction_value_median_and_save_to_table` component.\n",
    "\n",
    "The `to_job` method takes `retry_count` and `retry_pause_sec` arguments. Combining those arguments with `user_transaction_metrics_is_ready` lets you implement table check or sensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = bgq.Workflow(\n",
    "    definition=[\n",
    "        user_transaction_metrics_is_ready.to_job(\n",
    "            retry_count=100,\n",
    "            retry_pause_sec=40),\n",
    "        calculate_user_transaction_value_median_and_save_to_table.to_job()],\n",
    "    schedule_interval='* * * * *')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.run('2019-01-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment\n",
    "If you want to deploy this workflow, get rid of `run`s and `peek`s. Then, generate and deploy DAG in the same way you did in part 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Want more?\n",
    "If you want to discover more features, write to us **chibox-team@allegrogroup.com**. We will deliver next chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}