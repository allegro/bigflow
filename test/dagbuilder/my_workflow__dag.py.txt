# This file was generated by `bigflow build-dags`
# bigflow-workflow:  	my_workflow
# bigflow-build-ver: 	0.3.0
# bigflow-startdate: 	2020-07-02T10:00:00
# bigflow-imageid:   	europe-west1-docker.pkg.dev/my_docker_repository_project/my-project:0.3.0

import datetime
from airflow import DAG
from airflow import version

try:
    from airflow.kubernetes.secret import Secret
    from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import KubernetesPodOperator
except ImportError:
    # Fallback to older Airflow
    from airflow.contrib.kubernetes.secret import Secret
    from airflow.contrib.operators.kubernetes_pod_operator import KubernetesPodOperator

# To deploy BigFlow project, following requirements options are needed: (airflow 1.x + composer 1.x) or (airflow 2.x + composer >= 2.1.0)
IS_COMPOSER_2_X = version.version >= '2.0.0'
IS_AIRFLOW_2_3_X = version.version >= '2.3.0'
namespace = 'composer-user-workloads' if IS_COMPOSER_2_X else 'default'

default_args = dict(
    owner='airflow',
    depends_on_past=True,
    start_date=datetime.datetime(2020, 7, 2, 8, 0),
    email_on_failure=False,
    email_on_retry=False,
    execution_timeout=datetime.timedelta(seconds=10800),
)

dag = DAG(
    'my_workflow__v0_3_0__2020_07_02_10_00_00',
    default_args=default_args,
    max_active_runs=1,
    schedule_interval='@hourly',
)

tjob1_pod_operator_params = {
    'dag': dag,
    'task_id': 'job1',
    'name': 'job1',
    'cmds': ['python', '-m', 'bigflow', 'run'],
    'arguments': [
        '--job', 'my_workflow.job1',
        '--runtime', '{{ execution_date.strftime("%Y-%m-%d %H:%M:%S") }}',
        '--project-package', 'ca',
        '--config', '{{var.value.env}}',
    ],
    'namespace': namespace,
    'image': 'europe-west1-docker.pkg.dev/my_docker_repository_project/my-project:0.3.0',
    'is_delete_operator_pod': True,
    'retries': 10,
    'retry_delay': datetime.timedelta(seconds=20),
    'secrets': [],
    'execution_timeout': datetime.timedelta(seconds=10800),
}
if IS_AIRFLOW_2_3_X:
    tjob1_pod_operator_params['config_file'] = "/home/airflow/composer_kube_config"
    tjob1_pod_operator_params['kubernetes_conn_id'] = "kubernetes_default"

tjob1 = KubernetesPodOperator(**tjob1_pod_operator_params)


tjob2_pod_operator_params = {
    'dag': dag,
    'task_id': 'job2',
    'name': 'job2',
    'cmds': ['python', '-m', 'bigflow', 'run'],
    'arguments': [
        '--job', 'my_workflow.job2',
        '--runtime', '{{ execution_date.strftime("%Y-%m-%d %H:%M:%S") }}',
        '--project-package', 'ca',
        '--config', '{{var.value.env}}',
    ],
    'namespace': namespace,
    'image': 'europe-west1-docker.pkg.dev/my_docker_repository_project/my-project:0.3.0',
    'is_delete_operator_pod': True,
    'retries': 100,
    'retry_delay': datetime.timedelta(seconds=200),
    'secrets': [],
    'execution_timeout': datetime.timedelta(seconds=10800),
}
if IS_AIRFLOW_2_3_X:
    tjob2_pod_operator_params['config_file'] = "/home/airflow/composer_kube_config"
    tjob2_pod_operator_params['kubernetes_conn_id'] = "kubernetes_default"

tjob2 = KubernetesPodOperator(**tjob2_pod_operator_params)

tjob2.set_upstream(tjob1)

tjob3_pod_operator_params = {
    'dag': dag,
    'task_id': 'job3',
    'name': 'job3',
    'cmds': ['python', '-m', 'bigflow', 'run'],
    'arguments': [
        '--job', 'my_workflow.job3',
        '--runtime', '{{ execution_date.strftime("%Y-%m-%d %H:%M:%S") }}',
        '--project-package', 'ca',
        '--config', '{{var.value.env}}',
    ],
    'namespace': namespace,
    'image': 'europe-west1-docker.pkg.dev/my_docker_repository_project/my-project:0.3.0',
    'is_delete_operator_pod': True,
    'retries': 100,
    'retry_delay': datetime.timedelta(seconds=200),
    'secrets': [],
    'execution_timeout': datetime.timedelta(seconds=10800),
}
if IS_AIRFLOW_2_3_X:
    tjob3_pod_operator_params['config_file'] = "/home/airflow/composer_kube_config"
    tjob3_pod_operator_params['kubernetes_conn_id'] = "kubernetes_default"

tjob3 = KubernetesPodOperator(**tjob3_pod_operator_params)

tjob3.set_upstream(tjob2)
tjob3.set_upstream(tjob1)

